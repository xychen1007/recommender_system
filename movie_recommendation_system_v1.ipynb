{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agenda\n",
    "- 各自讲讲模型部分（怎么建的、有啥问题）\n",
    "- 一起讨论大的目标\n",
    "\n",
    "\n",
    "TODO\n",
    "- all: 顺着大方向，作一些调整，补充markdown解释\n",
    "- jinming: try implict ALS CF Model. Use MAE?\n",
    "- leyi: 改reco，加roc-based metrics, evaluate model 2\n",
    "\n",
    "ps: 避免漏掉修改，两个人合一份，最后再把两份整合到一起"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Business Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that you work for a digital media company that needs to recommend movies. What is your objective? What are you trying to optimizing and what are you willing to sacrifice? \n",
    "\n",
    "State your business objectives in building a recommendation system. Indicate\n",
    "\n",
    "- Objective function: state your business objective, and how this would translate to the metrics that you will ultimately be optimizing for\t \t \t \t\t\n",
    "- Any metric you wish to optimize in addition to accuracy \n",
    "- The intended user （active users with certain amount of rating records）\n",
    "- Any business rules you think will be important  （推荐内容的种类多样性？不重复性？但比较复杂，这次作业可能暂时实现不出来）\n",
    "- Performance requirements (time and/or complexity) （训练和预测时间要可接受，支持每天更新，支持大样本）\n",
    "- And more (for example: interpretability, diversity, novelty, etc) \t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初步想法：\n",
    "\n",
    "目标：首页推荐TOP K电影给每个用户\n",
    "\n",
    "实现：训练模型（min mae） -> 推荐电影给用户（max recall/auc, user_coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Based on the movie-rating historical data, we are to provide personalizied lists of movies specific to active user, in section like \"TOP 10 MOVIES THAT YOU MAY ALSO LIKE\" in the APP.\n",
    "\n",
    "- To goal is to make good recommendations to active users, in order to increase the chance of them clicking into, watching and liking one of the recommended movies. \n",
    "\n",
    "- Therefore, while maximizing rating accuracy for model training, our ultimate goal is to improve recall/AUC and user-coverage: the higher these metrics are, the better job we are doing in making well recommendations to active users. \n",
    "\n",
    "- To achieve this goal, we will limit our training set to only include movies and users with certain number of rating records, so that we could achieve better overall accuracy thus giving better recommendations. \n",
    "\n",
    "- Inevitably, this will limit the system's performance in cold-start recommendations to some extent but this is acceptable under this overall objective. So we might care less on metrics like movie-coverage, catalog-coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reco func可能要完善一下：推荐之前没看过的（就是没在train set里的）。更严格来讲，应该推荐某个日期之后的。但实现起来可能比较复杂。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Exploration & Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T14:51:45.388078Z",
     "start_time": "2020-11-12T14:51:44.385615Z"
    }
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from item_based_cf import *\n",
    "from metrics_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T14:51:28.947480Z",
     "start_time": "2020-11-12T14:51:22.130206Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./ml-latest/ratings.csv')\n",
    "df.drop(\"timestamp\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T14:51:10.303888Z",
     "start_time": "2020-11-12T14:51:04.270Z"
    }
   },
   "outputs": [],
   "source": [
    "n_users = df['userId'].nunique()\n",
    "n_items = df['movieId'].nunique()\n",
    "sparsity = round(1.0-len(df)/float(n_users*n_items),5)\n",
    "print(f'The number of unique users is {n_users}')\n",
    "print(f'The number of unique items is {n_items}')\n",
    "print(f'The sparsity is {sparsity*100}%')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "fig = sns.histplot(df['movieId'])\n",
    "ax.set_ylabel('Percentage of Users Rating the Movie', size=18)\n",
    "ax.set_xlabel('MovieID', size=18)\n",
    "ax.set_title('Ratings Distribution by Movies', size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cover movies of all popularity level: set some lower threshold for movies, like only include movies with greater than 5 or 100 ratings, but I wouldn't select only the most popular movies.\n",
    "\n",
    "If you include all levels of popularity it would be great to subset your accuracy results by popularity. Do you see a curve in accuracy from least to most popular?\n",
    "\n",
    "sample from sampled dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEYI:能不能这样，先不设定门槛，随机sample，这样可以包括所有popularity程度的电影。\n",
    "#然后将popularity分大概3层或者5层，分别看看每个层级的acc如何（例如一个柱状图or curve），最后结合这个结果决定\n",
    "#我们认为至少需要多少程度的电影，才能够获得比较好的准确度，所以暂时作出牺牲，不推荐那些相对太冷的电影给用户。\n",
    "#因为追求的是“reco the right movies to users\"，而不是要保证cover到所有popularity程度的电影"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Memory-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T08:35:03.549581Z",
     "start_time": "2020-11-09T08:34:54.422158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users: 169553\n",
      "number of items: 3929\n",
      "shape of sampled df: (23641130, 3)\n"
     ]
    }
   ],
   "source": [
    "##default param is 4300 and 6800, which can satisfy hw requirement, and it takes about 6 hours to train and predict\n",
    "df_sample = sample_df(df, min_item=20, min_user=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEYI: 可能需要稍微解释下为啥是这个thresholds？\n",
    "# 体感这个标准还是有点略严格，只包括评过2000+部电影的用户、被10000+个用户评过的电影，似乎有点冷启动不友好？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>128418</td>\n",
       "      <td>1272</td>\n",
       "      <td>1</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128419</td>\n",
       "      <td>1272</td>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128420</td>\n",
       "      <td>1272</td>\n",
       "      <td>11</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128421</td>\n",
       "      <td>1272</td>\n",
       "      <td>16</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128422</td>\n",
       "      <td>1272</td>\n",
       "      <td>21</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  rating\n",
       "128418    1272        1     3.5\n",
       "128419    1272        7     2.0\n",
       "128420    1272       11     4.0\n",
       "128421    1272       16     2.5\n",
       "128422    1272       21     3.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(df_sample, test_size=0.50)\n",
    "# train_set, valid_set = train_test_split(train_set, test_size=0.2)\n",
    "rating_matrix_train = train_set.pivot(index='movieId', columns='userId', values='rating')\n",
    "rating_matrix_test = test_set.pivot(index='movieId', columns='userId', values='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small = rating_matrix_train.iloc[:100,:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_small = rating_matrix_test.iloc[:100,:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemBasedCF():\n",
    "    def __init__(self, k_neighbors = 10, similarity_metrics = 'pearson'):\n",
    "        self.train_set = None\n",
    "        self.similarity_matrix = None\n",
    "        self.k_neighbors = k_neighbors\n",
    "        self.similarity_metrics = similarity_metrics\n",
    "\n",
    "    def fit(self, train_df, dummy_y=None):\n",
    "        train_df = train_set.pivot(index='movieId', columns='userId', values='rating')#修改输入格式\n",
    "        \n",
    "        self.train_set = train_df.values\n",
    "        self.similarity_matrix = sim_matrix(self.train_set, self.similarity_metrics)\n",
    "\n",
    "    def predict(self, test_df):\n",
    "        test_df = test_df.pivot(index='movieId', columns='userId', values='rating')\n",
    "        \n",
    "        pred_df = np.zeros(test_df.shape)\n",
    "        train_set_filled = np.nan_to_num(self.train_set)\n",
    "        nbrs = NearestNeighbors(n_neighbors=self.k_neighbors+1, algorithm='ball_tree').fit(train_set_filled)\n",
    "        distances, indices = nbrs.kneighbors(train_set_filled)\n",
    "        \n",
    "        # return average score of each movie with NAN ignored\n",
    "        mean_item = np.nanmean(self.train_set, axis=1)\n",
    "        for i in range(len(pred_df)):\n",
    "            pred_df[i][:] = mean_item[i]\n",
    "            neighbor_indice = indices[i][1:]\n",
    "            for j in range(len(pred_df[i])):\n",
    "                if np.isnan(self.train_set[i][j]):\n",
    "                    pred_df[i][j] += np.nansum(self.similarity_matrix[i][neighbor_indice] * (\n",
    "                                self.train_set[:, j][neighbor_indice] - mean_item[neighbor_indice])) / np.sum(\n",
    "                        self.similarity_matrix[i][neighbor_indice])\n",
    "                else:\n",
    "                    pred_df[i][j] = np.nan\n",
    "        return pred_df\n",
    "    \n",
    "    # requested methods for sk-learn estimator\n",
    "    def get_params(self, deep = True):\n",
    "        return {\"k_neighbors\": self.k_neighbors, \"similarity_metrics\": self.similarity_metrics}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEYI: put the adjusted ItemBasedCF to the python file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = ItemBasedCF(5, 'pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_time in seconds: 356.51662373542786\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model1.fit(rating_matrix_train)\n",
    "# model1.fit(train_small)\n",
    "end = time.time()\n",
    "print(f'training_time in seconds: {end-start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Model Tuning with Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEYI: save the models' final weights to local, for greater speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "pred_set = model1.predict(rating_matrix_test)\n",
    "# pred_set = model1.predict(test_small)\n",
    "end = time.time()\n",
    "print(f'training_time in seconds: {end-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEYI: Reasons to use MAE:\n",
    "\n",
    "One large characteristic of Mean Average Error(MAE) does not give any bias to extrema in error terms. If there are outliers or large error terms, it will weigh those equally to the other predictions. Therefore, MAE should be preferred when looking toward rating accuracy when you’re not really looking toward the importance of outliers. To get a holistic view or representation of the Recommender System, use MAE.\n",
    "\n",
    "https://towardsdatascience.com/evaluating-recommender-systems-root-means-squared-error-or-mean-absolute-error-1744abc2beac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reform loss function for gridsearch\n",
    "def My_MAE(pred, test):\n",
    "    try:\n",
    "        test = test.pivot(index='movieId', columns='userId', values='rating')\n",
    "    except:\n",
    "        pass\n",
    "    try: \n",
    "        pred = pred.pivot(index='movieId', columns='userId', values='rating')\n",
    "    except:\n",
    "        pass\n",
    "    return MAE(pred, test)\n",
    "scoring = make_scorer(MAE, greater_is_better = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'similarity_metrics':['pearson','cosine'], 'k_neighbors':np.arange(10,35,5)}\n",
    "model_test = ItemBasedCF()\n",
    "gs = GridSearchCV(model_test, parameters, scoring = scoring, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gs.fit(df_sample, df_sample) #改成train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k_neighbors': 10, 'similarity_metrics': 'pearson'}\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8884583973284366\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEYI: add some markdown here? 简单说说最后模型用了啥param，最优结果是多少"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the evaluation metrics change as a function of your model size? Systematically change your data set size by again sampling your data from a small size to a large size (25%, 50%, 75%, 100%):\n",
    "1. Does overall accuracy change?\n",
    "2. What about the distribution of accuracy over users or items?\n",
    "3. How does run-time scale with data size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: ADD PLOTS TO answer:\n",
    "#How does the evauation metrics change as sample size changes? like 25%, 50%, 75%, 100%\n",
    "#How does the training and testing runtime change as sample size changes? like 25%, 50%, 75%, 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### error-based accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6885897602816633"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE(rating_matrix_test, pred_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7899858612161919"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(rating_matrix_test, pred_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "\n",
    "def recall(test, pred): #test, pred是2维矩阵\n",
    "    return recall_score(test.reshape(-1), pred.reshape(-1))    \n",
    "\n",
    "def auc(test, pred):\n",
    "    return roc_auc_score(test.reshape(-1), pred.reshape(-1))\n",
    "\n",
    "def precision(test, pred):\n",
    "    return precision_score(test.reshape(-1), pred.reshape(-1))\n",
    "\n",
    "#测试可以用之后，放进metrics.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ranking-based accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.91174836018709"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcg(rating_matrix_test.values, pred_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOADD: ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_coverage: the fraction of users for which at least k items can be recommended well\n",
    "user_coverage = cal_coverage(test, pred_set, threshold=3, coverage_type = 'user')\n",
    "\n",
    "# item_coverage: the fraction of items that can be recommended to at least k users well\n",
    "item_coverage = cal_coverage(test, pred_set, threshold=3, coverage_type = 'item')\n",
    "\n",
    "# catalog_coverage: the fraction of items that are in the top-k for at least 1 user\n",
    "catalog_coverage = cal_coverage(test, pred_set, coverage_type = 'catalog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3)Scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add plots: runtime vs sample size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Other design choices to consider?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Model-based  Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T15:21:38.116502Z",
     "start_time": "2020-11-13T15:21:30.681090Z"
    }
   },
   "outputs": [],
   "source": [
    "#initialize spark \n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PySpark ML Basics\").getOrCreate()\n",
    "\n",
    "#process data into dataframe format\n",
    "\n",
    "df = spark.read.format('csv').options(header='true').load('./ml-latest/ratings.csv').toDF(\"userId\",  \"movieId\",  \"rating\",   \"timestamp\")\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "df = df.withColumn(\"userId\", df[\"userId\"].cast(IntegerType())). \\\n",
    "withColumn(\"movieId\", df[\"movieId\"].cast(IntegerType())). \\\n",
    "withColumn(\"rating\",df[\"rating\"].cast(FloatType())). \\\n",
    "withColumn(\"timestamp\", df['timestamp'].cast(LongType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T07:37:40.506509Z",
     "start_time": "2020-11-13T07:37:40.502363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: float (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T15:21:39.479025Z",
     "start_time": "2020-11-13T15:21:39.473076Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample_df_pyspark(df, min_item=20, min_user=1000):\n",
    "    \n",
    "    users = df.groupby('userId').count()\n",
    "    users= users.withColumn('num_of_movies',users['count']).select('userId','num_of_movies')\n",
    "    \n",
    "    movies = df.groupby('movieId').count()\n",
    "    movies = movies.withColumn('num_of_users',movies['count']).select('movieId','num_of_users')\n",
    "    \n",
    "    tmp = movies.filter(movies.num_of_users>=min_user).\\\n",
    "                join(df,df.movieId==movies.movieId).\\\n",
    "                select(df.userId,df.movieId,df.rating,df.timestamp)\n",
    "    sample = users.filter(users.num_of_movies>=min_item).\\\n",
    "                join(tmp,tmp.userId==users.userId).\\\n",
    "                select(tmp.userId,tmp.movieId,tmp.rating,tmp.timestamp)\n",
    "\n",
    "    return sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T15:21:40.067248Z",
     "start_time": "2020-11-13T15:21:39.871393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: float (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample = sample_df_pyspark(df, min_item=200, min_user=5000)\n",
    "df_sample.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T15:21:42.404672Z",
     "start_time": "2020-11-13T15:21:42.369559Z"
    }
   },
   "outputs": [],
   "source": [
    "#train test split \n",
    "train,test=df_sample.orderBy(rand()).randomSplit((0.8,0.2),seed=1234)\n",
    "\n",
    "#or read train/test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T15:24:01.239299Z",
     "start_time": "2020-11-13T15:21:44.901623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean absolute error = 0.6075575849407786\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(train)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "#改下MAE\n",
    "\n",
    "mae = evaluator.evaluate(predictions)\n",
    "print(\"mean absolute error = \" + str(mae))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T08:04:41.368438Z",
     "start_time": "2020-11-13T08:03:48.760237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+----------+\n",
      "|userId|movieId|rating|timestamp|prediction|\n",
      "+------+-------+------+---------+----------+\n",
      "| 10362|    471|   4.0|946845996| 3.4707136|\n",
      "| 10362|   1223|   4.0|946847119|  4.091524|\n",
      "| 10362|   1199|   3.0|946847985|   3.86697|\n",
      "| 10362|     34|   4.0|946845484|  4.084771|\n",
      "| 10362|   1198|   4.0|946848947| 4.1369057|\n",
      "| 10362|   1183|   3.0|946849928| 3.1311548|\n",
      "| 10362|    916|   4.0|946845664|  4.024228|\n",
      "| 10362|   2804|   4.0|946845452| 4.0380464|\n",
      "| 10362|   1208|   4.0|946849687| 3.7933345|\n",
      "| 10362|   1961|   3.0|946850183|   3.36571|\n",
      "| 10362|    539|   4.0|946847485| 2.9690156|\n",
      "| 10362|   1197|   5.0|946845620|  4.110145|\n",
      "| 10362|   2628|   3.0|946848528|  2.067631|\n",
      "| 10362|   1207|   4.0|946849507|  4.278883|\n",
      "| 10362|   2109|   4.0|946846291| 3.5137308|\n",
      "| 10362|   2193|   4.0|946848506|  2.702454|\n",
      "| 10362|    306|   4.0|946851003|  3.903015|\n",
      "| 10362|   1917|   2.0|946849223| 1.6949416|\n",
      "| 10362|   2485|   4.0|946846614| 2.3564868|\n",
      "| 10362|   2105|   3.0|946848506| 2.8676336|\n",
      "+------+-------+------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.filter(predictions.userId==10362).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEYI: can try the implicit ALS model\n",
    "#als = ALS(maxIter=5, regParam=0.01, implicitPrefs=True,\n",
    "#           userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Model Tuning with Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T05:42:56.619660Z",
     "start_time": "2020-11-14T05:10:51.843644Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder \n",
    "\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(als.maxIter,[5,10,20]) \\\n",
    "    .addGrid(als.regParam, [0.1, 0.01,0.001]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=als,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\"),\n",
    "                          numFolds=5)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T06:35:47.790119Z",
     "start_time": "2020-11-14T06:35:47.786786Z"
    }
   },
   "outputs": [],
   "source": [
    "paramMaps = cvModel.getEstimatorParamMaps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T06:48:07.150117Z",
     "start_time": "2020-11-14T06:48:07.145190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {Param(parent='ALS_1020de5001ff', name='maxIter', doc='max number of iterations (>= 0).'): 20, Param(parent='ALS_1020de5001ff', name='regParam', doc='regularization parameter (>= 0).'): 0.001}\n",
      "The miniest rmse: 0.7546022373461236\n"
     ]
    }
   ],
   "source": [
    "results = np.array(cvModel.avgMetrics)\n",
    "bestparams = paramMaps[np.argmin(results)]\n",
    "print(\"Best Params:\", bestparams)\n",
    "print(\"The miniest rmse:\", np.min(results)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T06:51:56.075195Z",
     "start_time": "2020-11-14T06:51:55.304367Z"
    }
   },
   "outputs": [],
   "source": [
    "#save best model \n",
    "model_path = \"./model\"\n",
    "cvModel.write().save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ROC-based metrics (recall, precision, roc curve, auc)\n",
    "- DCG\n",
    "- coverage\n",
    "- scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T15:50:37.447256Z",
     "start_time": "2020-11-13T15:50:37.398717Z"
    }
   },
   "outputs": [],
   "source": [
    "#sample users to evaluate \n",
    "test.createOrReplaceTempView(\"test\")\n",
    "predictions.createOrReplaceTempView(\"predictions\")\n",
    "sample_users = spark.sql(\"select distinct userId from test limit 1000\")\n",
    "sample_users.createOrReplaceTempView(\"sampleuser\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T09:32:42.803388Z",
     "start_time": "2020-11-13T09:32:42.548883Z"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.window import Window   \n",
    "\n",
    "# #get top 10 recommendations for sample users \n",
    "# userRecs = model.recommendForUserSubset(sample_users,10)\n",
    "# cleanedRecs=userRecs.select('userId',explode(userRecs.recommendations.movieId).alias(\"movieId\")).withColumn(\"y\",lit(1)) \n",
    "\n",
    "\n",
    "# #get top 10 rated movies of sample users \n",
    "# toprank = test.select(\"userId\",\"movieId\",\"rating\", row_number().\\\n",
    "#                        over(Window.partitionBy(\"userId\").\\\n",
    "#                             orderBy(desc(\"rating\"))).alias(\"rank\")).withColumn(\"y\",lit(1))\n",
    "# toprank.createOrReplaceTempView(\"toprank\")\n",
    "\n",
    "# useRats = spark.sql(\"select userId, movieId, y from toprank where userId in (select userId from sampleuser) and rank<=10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T15:40:00.070062Z",
     "start_time": "2020-11-13T15:39:09.580140Z"
    }
   },
   "outputs": [],
   "source": [
    "num_users= 1000\n",
    "num_items= df_sample.select('movieId').distinct().count() \n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "\n",
    "#Construct a lookup map (movieId to Long) \n",
    "movie_key_map = df_sample.select('movieId').distinct().rdd.map(lambda x:x[0]).zipWithIndex().collectAsMap()\n",
    "\n",
    "#Construct a lookup map (movieId to Long) \n",
    "user_key_map = df_sample.select('userId').rdd.map(lambda x:x[0]).zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T15:40:18.583020Z",
     "start_time": "2020-11-13T15:40:18.572008Z"
    }
   },
   "outputs": [],
   "source": [
    "def recodf2array(df,valcol,num_users=num_users,num_items=num_items): \n",
    "    \n",
    "    rows = np.concatenate(\n",
    "            df.select(\"userId\").rdd.glom().map(\n",
    "              lambda x: np.array([user_key_map.get(elem[0]) for elem in x]))\n",
    "            .collect())\n",
    "    \n",
    "    cols = np.concatenate(\n",
    "            df.select(\"movieId\").rdd.glom().map(\n",
    "              lambda x: np.array([movie_key_map.get(elem[0]) for elem in x]))\n",
    "            .collect())\n",
    "    \n",
    "    data = np.concatenate(\n",
    "            df.select(valcol).rdd.glom().map(\n",
    "              lambda x: np.array([elem[0] for elem in x]))\n",
    "            .collect())\n",
    "    \n",
    "    sparse_matrix = sparse.coo_matrix((data, (rows, cols)), \n",
    "                        shape=(num_users, num_items))\n",
    "    \n",
    "    sparse_matrix = sparse_matrix.tocsr() \n",
    "    \n",
    "    return sparse_matrix.toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T15:56:59.267941Z",
     "start_time": "2020-11-13T15:50:55.728865Z"
    }
   },
   "outputs": [],
   "source": [
    "#transform dataframe to matrix \n",
    "sub_test = spark.sql(\"select userId,movieId,rating from test where userId in (select userId from sampleuser)\")\n",
    "sub_predictions = spark.sql(\"select userId,movieId,prediction from predictions where userId in (select userId from sampleuser)\")\n",
    "test_matrix = recodf2array(sub_test,'rating')\n",
    "pred_matrix = recodf2array(sub_predictions,'prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dcg & ndcg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T05:07:16.849100Z",
     "start_time": "2020-11-14T05:07:16.577281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcg: 165.43502031840197\n",
      "ndcg: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"dcg:\", dcg(test_matrix,pred_matrix))\n",
    "print(\"ndcg:\", ndcg(test_matrix,pred_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- coverage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T05:09:29.348220Z",
     "start_time": "2020-11-14T05:09:29.260934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user coverage: 0.9285714285714286\n"
     ]
    }
   ],
   "source": [
    "print(\"user coverage:\", cal_coverage(test_matrix, pred_matrix, k_items=10, threshold=3, coverage_type='user')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ROC based metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-14T05:09:29.118988Z",
     "start_time": "2020-11-14T05:09:27.194623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5382440476190476, 0.766789923039923, 0.5382440476190476)\n"
     ]
    }
   ],
   "source": [
    "print(roc_metrics(test_matrix, pred_matrix, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Other design choices to consider?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Final Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• How does your recommendation system meet your hypothetical objectives? Would you feel comfortable putting these solutions into production at a real company? What would be the potential watch outs?\n",
    "\n",
    "• What is your final recommendation to your manager?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Final recommendation to your manager\n",
    "- Reasons and potential watch outs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#举例"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
